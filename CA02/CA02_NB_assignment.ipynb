{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZYXwtCsL_y"
   },
   "source": [
    "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm. \n",
    "\n",
    "In this assignment you will ...\n",
    "1. Complete the code such a way that it works correctly with this given parts of the program.\n",
    "2. Explain as clearly as possible what each part of the code is doing. Use \"Markdown\" texts and code commenting to explain the code\n",
    "\n",
    "IMPORTANT NOTE:\n",
    "\n",
    "The path of your data folders 'train-mails' and 'test-mails' must be './train-mails' and './test-mails'. This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive. The reason for doing this is, this way the peer reviewes and I would be able to run your code from our computers using this exact same relative path, irrespective of our folder hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/student/opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/student/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/student/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/student/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/student/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Users/student/opt/anaconda3/lib/python3.9/site-packages (1.9.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in /Users/student/opt/anaconda3/lib/python3.9/site-packages (from scipy) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_DvtT7sOIr",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING AND PREPARING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjKF0nIMwz8_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#1 - Constructing a dictionary for all words\n",
    "#2 - Removing items that are not an alphabetic words or items that are a single letter word\n",
    "#3 - Transforming the dictionary into a list of most common 3000 words\n",
    "\n",
    "def make_Dictionary(root_dir):\n",
    "  all_words = []\n",
    "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
    "  for mail in emails:\n",
    "    with open(mail) as m:\n",
    "      for line in m:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "  dictionary = Counter(all_words)\n",
    "  list_to_remove = list(dictionary)\n",
    "  for item in list_to_remove:\n",
    "    #Removing if not alphabetical\n",
    "    if item.isalpha() == False:\n",
    "      del dictionary[item]\n",
    "    #Removing if a single letter word\n",
    "    elif len(item) == 1:\n",
    "      del dictionary[item]\n",
    "    #Transforming the dictionary into a list of most common 3000 words\n",
    "  dictionary = dictionary.most_common(3000)\n",
    "  return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmVW5xNlyOFc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Creating a label and word frequency matrix using dictionary\n",
    "\n",
    "#The code is performing the following operations:\n",
    "    # - Extracting the third line of the file\n",
    "    # - Splitting the line into individual words\n",
    "    # - Counting the frequency of each word in the line and updating the features_matrix array accordingly\n",
    "    # - Determining if the email is spam (label 1) or not (label 0) based on the name of the file\n",
    "    # - Updating the train_labels array with the label determined in the previous step\n",
    "\n",
    "def extract_features(mail_dir):\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  train_labels = np.zeros(len(files))\n",
    "  count = 1;\n",
    "  docID = 0;\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i, line in enumerate(fi):\n",
    "        if i ==2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "        train_labels[docID] = 1;\n",
    "        count = count + 1\n",
    "      docID = docID + 1\n",
    "  return features_matrix, train_labels                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoq-rE7Mx0pp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Defining TRAIN_DIR variable and assigning it a string value with the file path to a directory on a file\n",
    "#system that contains email files for training. The variable TRAIN_DIR contains training data.\n",
    "TRAIN_DIR = 'Data/train-mails'\n",
    "\n",
    "#Defining TEST_DIR variable and assigning it a string value with the file path to a directory on a file\n",
    "#system that contains email files for testing. The variable TEST_DIR contains testing data.\n",
    "TEST_DIR = 'Data/test-mails'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n"
     ]
    }
   ],
   "source": [
    "#Defining a new variable (dictionary) and with make_Dictionary function creating a dictionary of words from\n",
    "#emails in the TRAIN_DIR file\n",
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
    "\n",
    "#Creating features_matrix and test_features_matrix variables that store the frequency of each unique word in each\n",
    "#email file and labels and test_labels variables that store the label (spam or not spam) of each email file.\n",
    "#Done for train and test data separately\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n",
      "Training Model using Gaussian Naive Bayes algorithm .....\n",
      "Training completed\n",
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
      "0.9653846153846154\n"
     ]
    }
   ],
   "source": [
    "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
    "\n",
    "#implementing the Gaussian Naive Bayes algorithm for classification\n",
    "#GaussianNB is used in classification and it assumes that features follow a normal distribution\n",
    "print (\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
    "model = GaussianNB()\n",
    "print (\"Training completed\")\n",
    "\n",
    "#training the model\n",
    "model.fit(features_matrix, labels)\n",
    "\n",
    "#predicting\n",
    "print (\"testing trained model to predict Test Data labels\")\n",
    "predicted_labels = model.predict(test_features_matrix)\n",
    "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
    "\n",
    "#evaluating performance (accuracy) - comparing the accuracy score for predicted labels (accuracy is a percentage of\n",
    "#correct predictions that our model got right)\n",
    "print(accuracy_score(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
